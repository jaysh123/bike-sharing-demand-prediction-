{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "9PIHJqyupx6M",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaysh123/bike-sharing-demand-prediction-/blob/main/Copy_of_Bike_Sharing_demand_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "# <font size='6px'><font color='#FF3206'>**Project Name**    - <font color='#8A2BE2'>Bike Sharing Demand Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "#        **Name**         - JAYSHREE\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://miro.medium.com/v2/resize:fit:474/1*S47bBeEK0SvlOY1jJXQjbg.jpeg',width=900)\n"
      ],
      "metadata": {
        "id": "BHj0ojqLCXaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike demand prediction is a common problem faced by bike rental companies, as accurately forecasting the demand for bikes can help optimize inventory and pricing strategies. In this project, I aim to **develop a regression supervised machine learning model** to predict the demand for bikes in a given time period.\n",
        "\n",
        "Originally dataset of bike rental information from a bike sharing company, had information including details on the number of bikes rented, the time and date of the rental, and various weather and seasonality features, information on other relevant factors that could impact bike demand, such as holidays, functioning or non functioning day.\n",
        "\n",
        "After **preprocessing** and cleaning the data, I split it into training and test sets and used the training data to train our machine learning model. I experimented with several different** model architectures and hyperparameter, ultimately selecting the model that performed the best on the test data.\n",
        "\n",
        "To evaluate the performance of our model, I used a variety of metrics, including mean absolute error, root mean squared error, and R-squared. I found that our model was able to make highly accurate predictions, with an** R-squared value of 0.88 and a mean absolute error of just 2.58.**\n",
        "\n",
        "In addition to evaluating the performance of our model on the test data, I also conducted a series of** ablation studies** to understand the impact of individual features on the model's performance. I found that the temperature, as well as the weather and seasonality features, had the greatest impact on bike demand."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP9D-4O_2YBi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently **Rental bikes are introduced in many urban cities for the enhancement of mobility comfort**. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n",
        "\n",
        "**My goal** is to develop a model that is highly accurate, with a low mean absolute error and a high R-squared value. The model should also be able to provide insights into the factors that most impact bike demand, helping the bike sharing company to make data-driven decisions about how to optimize their operations."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "#data visualization libraries(matplotlib,seaborn, plotly)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# Datetime library for manipulating Date columns.\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "# from sci-kit library scaling, transforming and labeling functions are brought\n",
        "# which is used to change raw feature vectors into a representation that is more\n",
        "# suitable for the downstream estimators.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "# Importing various machine learning models.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Import different metrics from sci-kit libraries for model evaluation.\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Importing warnings library. The warnings module handles warnings in Python.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount The Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9hlMMIY613kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "bike_df=pd.read_csv('/content/drive/MyDrive/Copy of SeoulBikeData.csv',encoding ='latin')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look Viewing the data of top 5 rows to take a glimps of the data\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the data of bottom 5 rows to take a glimps of the data\n",
        "bike_df.tail()"
      ],
      "metadata": {
        "id": "bAPmW5fm4Yvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(bike_df.shape)\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting all the columns\n",
        "print(\"Features of the dataset:\")\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "TZEgsxre4nsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "#check details about the data set\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "print(f\"Data is duplicated ? {bike_df.duplicated().value_counts()},unique values with {len(bike_df[bike_df.duplicated()])} duplication\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "missing = pd.DataFrame((bike_df.isnull().sum())*100/bike_df.shape[0]).reset_index()\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.pointplot(x='index', y=0, data=missing)\n",
        "plt.xticks(rotation=90, fontsize=7)\n",
        "plt.title(\"Percentage of Missing values\")\n",
        "plt.ylabel(\"PERCENTAGE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DleWRNHm8SYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checking Null Value by plotting Heatmap\n",
        "\n",
        "sns.heatmap(bike_df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "*  There are 8760 observation and 14 features.\n",
        "\n",
        "*   In a day we have 24 hours and we have 365 days a year so 365 multiplied by 24 = 8760, which represents the number of line in the dataset\n",
        "\n",
        "*   There are no null values\n",
        "\n",
        "\n",
        "\n",
        "*  Dataset has all unique values i.e., there is no duplicate, which means data is free from bias as duplicates which can cause problems in downstream analysis, such as biasing results or making it difficult to accurately summarize the data.\n",
        "*  Date has some object data types, it should be datetime data type.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f'Features: {bike_df.columns.to_list()}')"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bike_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Breakdown of Our Features:**\n",
        "\n",
        "**Date **: The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str, we need to convert into datetime format.\n",
        "\n",
        "**Rented Bike Count** : Number of rented bikes per hour which our dependent variable and we need to predict that, type : int\n",
        "\n",
        "**Hour**: The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.\n",
        "\n",
        "**Temperature(°C)**: Temperature in Celsius, type : Float\n",
        "\n",
        "**Humidity(%)**: Humidity in the air in %, type : int\n",
        "\n",
        "**Wind speed (m/s)** : Speed of the wind in m/s, type : Float\n",
        "\n",
        "**Visibility (10m)**: Visibility in m, type : int\n",
        "\n",
        "**Dew point temperature(°C)**: Temperature at the beggining of the day, type : Float\n",
        "\n",
        "**Solar Radiation (MJ/m2)**: Sun contribution, type : Float\n",
        "\n",
        "**Rainfall(mm)**: Amount of raining in mm, type : Float\n",
        "\n",
        "**Snowfall (cm)**: Amount of snowing in cm, type : Float\n",
        "\n",
        "**Seasons**: *Season of the year, type : str, there are only 4 season's in data *.\n",
        "\n",
        "**Holiday**: If the day is holiday period or not, type: str\n",
        "\n",
        "**Functioning Day**: If the day is a Functioning Day or not, type : str"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# bike_df.nunique()\n",
        "for i in bike_df.columns.tolist():\n",
        "  print(f\"No. of unique values in {i} is {bike_df[i].nunique()}.\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.**Dataset Wranggling**"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset if not already loaded\n",
        "# bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# 1. Data Exploration\n",
        "# Display basic information about the dataset\n",
        "print(bike_df.info())\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(bike_df.head())\n",
        "\n",
        "# 2. Data Cleaning\n",
        "# Handle missing values, if any\n",
        "bike_df.dropna(inplace=True)  # Remove rows with missing values\n",
        "\n",
        "# 3. Data Analysis\n",
        "# Summary statistics\n",
        "print(bike_df.describe())\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = bike_df.corr()\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Data Visualization\n",
        "# Pairplot for visualizing relationships between numerical features\n",
        "sns.pairplot(bike_df, vars=['Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)'], hue='Seasons')\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "# 4. Feature Engineering\n",
        "# You can create new features or transform existing ones if needed.\n",
        "\n",
        "# 5. Model Preparation\n",
        "# Split the data into features (X) and the target variable (y)\n",
        "X = bike_df.drop(columns=['Rented Bike Count'])\n",
        "y = bike_df['Rented Bike Count']\n",
        "\n",
        "# 6. Data Splitting\n",
        "# Split the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GNLDqFkIAWNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common Data Wrangling and Analysis Tasks:**\n",
        "\n",
        "1.  **Data Cleaning:**\n",
        "\n",
        "    Handling missing values by imputation or removal.\n",
        "    Identifying and addressing outliers.\n",
        "    Dealing with duplicate records, if any.\n",
        "    Ensuring data types are correctly set for each column (e.g., date columns as datetime).\n",
        "2.  **Feature Engineering:**\n",
        "    Creating new features based on existing ones, such as date-related features (day of the week, month).\n",
        "\n",
        "    Normalizing or scaling numerical features.\n",
        "    Encoding categorical variables, often using one-hot encoding.\n",
        "    Handling text data through tokenization or text preprocessing if relevant.\n",
        "3.   **Data Exploration:**\n",
        "   Calculating summary statistics (mean, median, standard deviation).\n",
        "\n",
        "   Visualizing data distributions and relationships among features.\n",
        "   Exploring the correlation between features and the target variable.\n",
        "5.  **Feature Selection:**\n",
        "   Selecting relevant features based on domain knowledge and data analysis.\n",
        "\n",
        "   Using statistical tests or machine learning algorithms for feature selection.\n",
        "6. **Model Building and Evaluation:**\n",
        "  Developing predictive models for the target variable (e.g., bike rental counts).\n",
        "\n",
        "  **Insights:**\n",
        "\n",
        "  Identifying factors that strongly influence bike rental counts.\n",
        "\n",
        "  Discovering patterns and trends in bike rental data.\n",
        "\n",
        "  Assessing the impact of seasonality and external factors on bike rental demand.\n",
        "  \n",
        "  Understanding correlations between features and bike rental counts.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFbdT-guehLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Duplicate values**"
      ],
      "metadata": {
        "id": "C6eFQrTGetpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is it important to remove duplicate records from my data?**\n",
        "\n",
        "    \n",
        "\n",
        "*   \"Duplication\" just means that you have repeated data in your dataset. This could be due to things like data entry errors or data collection methods. By removing duplication in our data set, Time and money are saved by not sending identical communications multiple times to the same person.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4AZXWI2Gey8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Duplicate Values\n",
        "value=len(bike_df[bike_df.duplicated()])\n",
        "print(\"The number of duplicate values in the data set is = \",value)"
      ],
      "metadata": {
        "id": "HpCN4WDtfRtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* In the above data after count the missing and duplicate value we came to know that there are no missing and duplicate value present.\n",
        "\n",
        "\n",
        "*   Some of the columns name in the dataset are too large and clumsy so we change them into some simple name, and it don't affect our end results.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lb7WrJL2fjIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Changing column name**"
      ],
      "metadata": {
        "id": "KmdN1QPSgAdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename the complex columns name\n",
        "bike_df=bike_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "dkM7ySX5gJEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Python read \"Date\" column as a object type basically it reads as a string, as the date column is very important to analyze the users behaviour so we need to convert it into datetime format then we split it into 3 column i.e 'year', 'month', 'day'as a category data type.\n",
        "\n"
      ],
      "metadata": {
        "id": "azTi1AcvgS8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breaking date column**"
      ],
      "metadata": {
        "id": "csMplZgBgeyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n",
        "bike_df['Date'] = bike_df['Date'].str.replace('-', '/')\n",
        "bike_df['Date'] = bike_df['Date'].apply(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))\n"
      ],
      "metadata": {
        "id": "TzHn-OR9g0-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['year'] = bike_df['Date'].dt.year\n",
        "bike_df['month'] = bike_df['Date'].dt.month\n",
        "bike_df['day'] = bike_df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "LSX4IPspg3A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "bike_df['weekdays_weekend']=bike_df['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "bike_df=bike_df.drop(columns=['Date','day','year'],axis=1)"
      ],
      "metadata": {
        "id": "_nMT53Zmg7xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",\"day\".\n",
        "\n",
        "\n",
        "*  The \"year\" column in our data set is basically contain the 2 unique number contains the details of from 2017 december to 2018 november so if i consider this is a one year then we don't need the \"year\" column so we drop it.\n",
        "\n",
        "*   The other column \"day\", it contains the details about the each day of the month, for our relevence we don't need each day of each month data but we need the data about, if a day is a weekday or a weekend so we convert it into this format and drop the \"day\" column.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QpWSCDVLhFji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.head()"
      ],
      "metadata": {
        "id": "qp1uB5c_haKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['weekdays_weekend'].value_counts()"
      ],
      "metadata": {
        "id": "-lhzUw20hcRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTz5K7tshjZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Changing data type**"
      ],
      "metadata": {
        "id": "C8xsav9hhoSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **As \"Hour\",\"month\",\"weekdays_weekend\" column are show as a integer data type but actually it is a category data tyepe. so we need to change this data tyepe if we not then, while doing the further anlysis and correleted with this then the values are not actually true so we can mislead by this.**\n",
        "\n"
      ],
      "metadata": {
        "id": "w9yT5bo6hudh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the int64 column into catagory column\n",
        "cols=['Hour','month','weekdays_weekend']\n",
        "for col in cols:\n",
        "  bike_df[col]=bike_df[col].astype('category')"
      ],
      "metadata": {
        "id": "o2cFMmOliEs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's check the result of data type\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "g1KaeWF9iKDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.columns"
      ],
      "metadata": {
        "id": "Pa167YHoiRqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis Of The Data Set**"
      ],
      "metadata": {
        "id": "fPj4jrevhzkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Why do we perform EDA?**\n",
        "\n",
        "*  An EDA is a thorough examination meant to uncover the underlying structure of a data set and is important for a company because it exposes trends, patterns, and relationships that are not readily apparent.\n",
        "\n"
      ],
      "metadata": {
        "id": "wZ0OscZYh6B-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**"
      ],
      "metadata": {
        "id": "k_SBYFaYiJxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do you do univariate analysis?**\n",
        "\n",
        "* The key objective of Univariate analysis is to simply describe the data to find patterns within the data.  \n",
        "\n"
      ],
      "metadata": {
        "id": "cA86TIpkiQIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis of Dependent Variable:**"
      ],
      "metadata": {
        "id": "gIK0RhGOicsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**What is a dependent variable in data analysis?**\n",
        "\n",
        "\n",
        "*   we analyse our dependent variable,A dependent variable is a variable whose value will change depending on the value of another variable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E6bjsT3gihbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysation of categorical variables**"
      ],
      "metadata": {
        "id": "-xJgpyEXiu6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Our dependent variable is \"Rented Bike Count\" so we need to analysis this column with the other columns by using some visualisation plot.first we analyze the category data type then we proceed with the numerical data type**\n",
        "\n"
      ],
      "metadata": {
        "id": "GTRTbYsti3XC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **MONTH**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,7))\n",
        "sns.barplot(data=bike_df,x='month',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Month')\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart analysing the month variable"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above bar plot we can clearly say that, from the month 5 to 10 (May - October) the demand of the rented bike is high as compare to other months.These months came inside the summer season.Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Here are some ways insights can lead to positive business impact:\n",
        "\n",
        "   Optimizing Inventory: If the analysis shows that bike rentals are significantly higher in certain months or during specific times of the day, the business can optimize their inventory and staffing accordingly, reducing costs and improving customer satisfaction.\n",
        "\n",
        "\n",
        "*   Marketing and Promotions: Insights on when and why bike rentals increase or decrease can inform marketing strategies. For instance, offering discounts during low-demand months or creating special promotions during peak periods can attract more customers.\n",
        "\n",
        "\n",
        "* **Regarding insights that could lead to negative growth** , it is essential to consider the context and how insights are applied. For example, if the analysis reveals a significant drop in bike rentals during a specific month due to poor weather conditions, this negative insight can be addressed positively. The business can plan for seasonally adjusted pricing or offer alternative services during unfavorable weather to mitigate losses.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **weekdays_weekend**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "fig,ax=plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df,x='weekdays_weekend',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekenday ')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,7))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='weekdays_weekend',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekend ')"
      ],
      "metadata": {
        "id": "cPkEg5zqjuMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This chart is visualize the count of rented bikes according to the \"weekdays_weekend\" variable."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   From the above point plot and bar plot we can say that, in the week days which represent in blue colur show that the demand of the bike higher because of the office.\n",
        "\n",
        "*   Peak Time are 7 am to 9 am and 5 pm to 7 pm*\n",
        "*   The orange colur represent the weekend days, and it show that the demand of rented bikes are very low specially in the morning hour but when the evening start from 4 pm to 8 pm the demand slightly increases.*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "*   **Positive Business Impact:**\n",
        "\n",
        "Optimal Pricing Strategies: Analysis of bike rentals by the hour and day of the week can inform pricing strategies. For example, increasing prices during peak hours or offering discounts during off-peak hours can help maximize revenue.\n",
        "\n",
        "*   **However, it's crucial to note that insights themselves are not inherently positive or negative.** It's how the business responds to these insights that determines the outcome. Even insights that might initially seem negative can be turned into opportunities for improvement if handled strategically.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hour**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig,ax=plt.subplots(figsize=(12,7))\n",
        "sns.barplot(data=bike_df,x='Hour',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Hour ')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is visualize the count of rented bikes according to the \"Hour\" variable."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   In the above plot which shows, the use of rented bike according the hours and the data are from all over the year.\n",
        "*   generally people use rented bikes during their working hour from 7am to 9am and 5pm to 7pm.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "Optimal Resource Allocation: Understanding the peak hours of bike rentals can help the business allocate resources efficiently, such as having more staff and bikes available during those high-demand hours.\n",
        "\n",
        "\n",
        "\n",
        "*  In summary, insights from data analysis can be leveraged to create a positive business impact when used strategically. Even what may seem like negative insights can be turned into opportunities for improvement if the business responds effectively and adapts its strategies accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Functioning Day"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df,x='Functioning_Day',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,7))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Functioning_Day',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "metadata": {
        "id": "Nf492ktdx3jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is visualize the count of rented bikes according to the \"Functioning_Day\"."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  In the above bar plot and point plot which shows the use of rented bike in functioning day or non functioning day, and it clearly shows that,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Peoples don't use reneted bikes in no functioning day.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Positive Business Impact:**\n",
        "\n",
        "Resource Allocation: Understanding how bike rentals vary by hour and whether it's a functioning day or not can help the business allocate resources efficiently. For instance, more staff and bikes can be made available during peak hours on functioning days to meet the higher demand.\n",
        "*   **In the context of your visualization, if the data shows **a lower count of rented bikes on non-functioning days or during certain hours, this might initially be seen as a** negative insight**. However, the business can respond positively by developing strategies to attract more customers during those times or by optimizing costs during low-demand periods.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seasons"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "fig,ax=plt.subplots(figsize=(12,6))\n",
        "sns.barplot(data=bike_df,x='Seasons',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Seasons ')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,6))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Seasons',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to seasons ')"
      ],
      "metadata": {
        "id": "5LG4y5uPzBjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This chart is visualize the count of rented bikes according to the \"Hour\" and \"Seasons\" variables."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   In the above bar plot and point plot which shows, the use of rented bike in four different seasons, and it clearly shows that,\n",
        "\n",
        "*  In summer season the use of rented bike is high and peak time is 7am-9am and 5pm-7pm.\n",
        "*   In winter season the use of rented bike is very low maybe because of snowfall, fog, cold etc.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Positive Business Impact:**\n",
        "\n",
        "Optimized Inventory Management: Understanding how bike rentals vary by the hour and across seasons can help the business optimize its inventory. More bikes can be made available during peak hours and seasons with higher demand, while resources can be adjusted during off-peak times and seasons.\n",
        "\n",
        "\n",
        "\n",
        "*   However, as** with any insights, their impact depends on how the business responds.** It's crucial to understand that insights themselves are not inherently positive or negative; it's the strategic response that determines the outcome.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Holiday**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig,ax=plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df,x='Holiday',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Holiday ')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,6))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Holiday',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Holiday ')"
      ],
      "metadata": {
        "id": "D5iwZhbf2AYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is visualize the count of rented bikes according to the \"Holiday\" variable."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   In the above bar plot and point plot which shows the use of rented bike in a holiday, and it clearly shows that,\n",
        "In holiday, people uses the rented bike from 2pm-8pm.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyze of Numerical variables**"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Numerical Data\n",
        "\n",
        "\n",
        "\n",
        "* Numerical data is a data type expressed in numbers, rather than natural\n",
        "language description. Sometimes called quantitative data, numerical data is always collected in number form. Numerical data differentiates itself from other number form data types with its ability to carry out arithmetic operations with these numbers."
      ],
      "metadata": {
        "id": "7FeZj9Kw4OSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pays little attention to the skewness of our numerical features**"
      ],
      "metadata": {
        "id": "qX7Vy0Hf4swl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate numerical features from the dataframe\n",
        "numeric_features= bike_df.select_dtypes(exclude=['object','category'])\n",
        "numeric_features"
      ],
      "metadata": {
        "id": "R94H-_TP46-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# printing displots to analyze the distribution of all numerical features\n",
        "\n",
        "n=1\n",
        "plt.figure(figsize=(15,10))\n",
        "for i in numeric_features.columns:\n",
        "  plt.subplot(3,3,n)\n",
        "  n=n+1\n",
        "  sns.distplot(bike_df[i])\n",
        "  plt.title(i)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is generating a set of distribution plots (distplots) for all numerical features in your dataset. This chart is a valuable choice for several reasons:"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Right skewed columns are**\n",
        "\n",
        "Rented Bike Count (Its also our Dependent variable), Wind speed (m/s), Solar Radiation (MJ/m2), Rainfall(mm), Snowfall (cm),"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left skewed columns are**\n",
        "\n",
        "Visibility (10m), Dew point temperature(°C)\n",
        "\n"
      ],
      "metadata": {
        "id": "AmLwQnzQ9Vyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Positive Business Impact:**\n",
        "\n",
        "Data Understanding: Visualizing the distribution of numerical features can help in understanding the data and its characteristics. This understanding is a fundamental step in any data-driven decision-making process.\n",
        "\n",
        "\n",
        "*   **the insights gained from visualizing the distribution o**f numerical features are valuable for understanding the data and improving data quality. While some insights may initially appear negative, they offer opportunities for data enhancement, which can ultimately have a positive impact on business growth by leading to more accurate models and better-informed decisions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets try to find how is the relation of numerical features with our dependent variable**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numerical VS Rented Bike Count**"
      ],
      "metadata": {
        "id": "XspCVLre-Zy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Temperature\"\n",
        "bike_df.groupby('Temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This chart is generating a line plot to analyze the relationship between \"Rented_Bike_Count\" and \"Temperature\" by plotting the average \"Rented_Bike_Count\" for each unique temperature value."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we see that, people like to ride bikes when it is pretty hot around 25°C in average"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "**Temperature and Demand Correlation**: If the analysis shows a positive correlation between temperature and bike rentals (i.e., as temperature increases, bike rentals increase), this is a positive insight. The business can use this information to optimize operations, increase bike inventory during warmer seasons, and adjust pricing strategies to capitalize on higher demand.\n",
        "\n",
        "\n",
        "  **Negative Growth Mitigation:**\n",
        "\n",
        "**Temperature Extremes**: If the analysis shows that extremely hot or cold temperatures lead to a decrease in bike rentals, this could be seen as a negative insight. However, it also presents opportunities for mitigation. The business can plan special promotions during extreme temperature conditions to encourage rentals, provide weather-specific services, or diversify into other products or services during off-peak periods.\n",
        "\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Dew_point_temperature\"\n",
        "bike_df.groupby('Dew_point_temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyie_TGV_nxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This chart is generates a line plot to analyze the relationship between \"Rented_Bike_Count\" and \"Dew_point_temperature\" by plotting the average \"Rented_Bike_Count\" for each unique \"Dew_point_temperature\" value."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot of \"Dew_point_temperature', is almost same as the 'temperature' there is some similarity present we can check it in our next step."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "**Weather-Sensitive Operations**: If the analysis shows a correlation between dew point temperature and bike rentals, it's a positive insight. The business can use this information to optimize operations based on weather conditions. For example, it can plan promotions or services that encourage bike rentals on days with specific dew point temperature ranges.\n",
        "\n",
        "**Negative Growth Mitigation:**\n",
        "\n",
        "**Weather-Dependent Challenges**: If the analysis reveals that extreme dew point temperatures lead to a decrease in bike rentals, this might be initially seen as a negative insight. However, it also presents opportunities for mitigation. The business can plan for alternative services or diversify its offerings during extreme conditions."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Solar_Radiation\"\n",
        "bike_df.groupby('Solar_Radiation').mean()['Rented_Bike_Count'].plot()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is generates a line plot to analyze the relationship between \"Rented_Bike_Count\" and \"Solar_Radiation\" by plotting the average \"Rented_Bike_Count\" for each unique \"Solar_Radiation\" value. Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  from the above plot we see that, the amount of rented bikes is huge, when there is solar radiation, the count of rents is around 1000\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "**Weather-Dependent Operations:** If the analysis shows a correlation between solar radiation and bike rentals, it's a positive insight. The business can use this information to optimize operations based on weather conditions. For example, it can plan promotions or services that encourage bike rentals on days with specific solar radiation levels, attracting more customers during favorable weather.\n",
        "\n",
        "**Negative Growth Mitigation:**\n",
        "\n",
        "**Weather-Dependent Challenges**: If the analysis reveals that extreme solar radiation levels lead to a decrease in bike rentals, this might be seen as a negative insight. However, it also presents opportunities for mitigation. The business can plan for alternative services, promotions, or operational adjustments during extreme conditions to stimulate demand."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Snowfall\"\n",
        "bike_df.groupby('Snowfall').mean()['Rented_Bike_Count'].plot()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is generates a line plot to analyze the relationship between \"Rented_Bike_Count\" and \"Snowfall\" by plotting the average \"Rented_Bike_Count\" for each unique \"Snowfall\" value."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  We can see from the plot that, on the y-axis, the amount of rented bike is very low. When we have more than 4 cm of snow, the bike rents is much lower\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "**Seasonal Adjustments**: If the analysis reveals that there's a seasonal pattern, with decreased bike rentals during snowy conditions, it's a positive insight. The business can prepare for the seasonal variation by optimizing resources and offering winter-specific services or promotions.\n",
        "\n",
        "**Negative Growth Mitigation:**\n",
        "\n",
        "**Snow-Dependent Challenges**: If the analysis shows that snowfall leads to a significant decrease in bike rentals, this might be initially seen as a negative insight. However, it also presents opportunities for mitigation. The business can explore alternative services, such as winter gear rentals or bike storage, to mitigate the impact of reduced bike rentals during snowy conditions."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Rainfall\"\n",
        "bike_df.groupby('Rainfall').mean()['Rented_Bike_Count'].plot()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This is generates a line plot to analyze the relationship between \"Rented_Bike_Count\" and \"Rainfall\" by plotting the average \"Rented_Bike_Count\" for each unique \"Rainfall\" value."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above plot that, even if it rains a lot the demand of of rent bikes is not decreasing, here for example even if we have 20 mm of rain there is a big peak of rented bikes"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "**Weather-Dependent Operations**: If the analysis shows a correlation between rainfall and bike rentals, it's a positive insight. The business can use this information to optimize operations based on weather conditions. For example, it can plan promotions or services that encourage bike rentals on days with specific rainfall levels, attracting more customers during favorable weather.\n",
        "\n",
        "**Negative Growth Mitigation:**\n",
        "\n",
        "**Weather-Dependent Challenges**: If the analysis reveals that high rainfall levels lead to a decrease in bike rentals, this might be seen as a negative insight. However, it also presents opportunities for mitigation. The business can plan for alternative services, promotions, or operational adjustments during rainy conditions to stimulate demand."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Wind_speed\"\n",
        "bike_df.groupby('Wind_speed').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This is generates a line plot to analyze the relationship between \"Rented_Bike_Count\" and \"Wind_speed\" by plotting the average \"Rented_Bike_Count\" for each unique \"Wind_speed\" value."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above plot that, the demand of rented bike is uniformly distribute despite of wind speed but when the speed of wind is 7 m/s then the demand of bike also increase that clearly means people love to ride bikes when its little windy."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "**Weather-Dependent Operations**: If the analysis shows a correlation between wind speed and bike rentals, it's a positive insight. The business can use this information to optimize operations based on weather conditions. For example, it can plan promotions or services that encourage bike rentals on days with specific wind speeds, attracting more customers during favorable weather.\n",
        "\n",
        "**Negative Growth Mitigation:**\n",
        "\n",
        "**Weather-Dependent Challenges**: If the analysis reveals that high wind speeds lead to a decrease in bike rentals, this might be seen as a negative insight. However, it also presents opportunities for mitigation. The business can plan for alternative services, promotions, or operational adjustments during windy conditions to stimulate demand."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "#checking correlation using heatmap\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(bike_df.corr(),cmap='PiYG',annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **A correlation Heatmap is a type of graphical representation that displays the correlation matrix, which helps to determine the correlation between different variables**.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can observe on the heatmap that on the target variable line, the most positively correlated variables to the rent are:**\n",
        "\n",
        "\n",
        "*   the temperature\n",
        "\n",
        "*   the dew point temperature\n",
        "\n",
        "\n",
        "*   the solar radiation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And most negatively correlated variables are:**\n",
        "\n",
        "\n",
        "*   humidity\n",
        "\n",
        "*   rainfall\n",
        "\n"
      ],
      "metadata": {
        "id": "W16G1fmJEaeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **From the above correlation heatmap, We see that there is a positive correlation between columns 'Temperature' and 'Dew point temperature' i.e 0.91 so even if we drop this column then it won't affect the outcome of our analysis. And they have the same variations, so we can drop the column 'Dew point temperature(°C)'.**\n",
        "\n"
      ],
      "metadata": {
        "id": "BRJlKEU1EwSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the Dew point temperature column\n",
        "bike_df=bike_df.drop(['Dew_point_temperature'],axis=1)"
      ],
      "metadata": {
        "id": "26xi8fPlE2na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "ii5C4hkpFBj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "numeric_features = bike_df[['Temperature',  'Humidity', 'Wind_speed', 'Visibility', 'Solar_Radiation', 'Rainfall', 'Snowfall', 'Rented_Bike_Count']]\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(numeric_features)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***A pair plot, also known as a scatterplot matrix, is a useful visualization for exploring relationships between multiple numerical variables in a dataset. ***"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   we display the pair plot using Matplotlib.\n",
        "\n",
        "This pair plot will display scatterplots for all pairs of numerical features in the dataset, allowing you to visualize relationships and correlations between variables. It can help you identify patterns and relationships that may be useful for bike sharing demand prediction and analysis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement: The average count of rented bikes is higher during the summer season compared to the winter season.**"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   **Null Hypothesis (H0): The mean bike count during the summer season is equal to or less than the mean bike count during the winter season.**\n",
        "\n",
        "\n",
        "\n",
        "*  **Alternative Hypothesis (H1): The mean bike count during the summer season is higher than the mean bike count during the winter season.**\n"
      ],
      "metadata": {
        "id": "rGIgNNO4c0TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "summer_data = bike_df[bike_df['Seasons'] == 'Summer']['Rented_Bike_Count']\n",
        "winter_data = bike_df[bike_df['Seasons'] == 'Winter']['Rented_Bike_Count']\n",
        "\n",
        "# Perform a two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(summer_data, winter_data)\n",
        "\n",
        "# Set the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "    print(\"The average count of rented bikes is higher during the summer season compared to the winter season.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis\")\n",
        "    print(\"There is no significant difference in bike rental counts between the summer and winter seasons.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the p-value for Hypothesis 1, I used a two-sample t-test. A t-test is a statistical test used to determine whether there is a significant difference between the means of two groups or samples. In this case, I performed a two-sample t-test to compare the mean bike counts between the summer season and the winter season. The p-value obtained from this t-test helps determine whether there is a significant difference in bike rental counts between these two seasons.\n",
        "\n",
        "The p-value obtained from the t-test is then used to make a decision about the null hypothesis. If the p-value is less than the chosen significance level (alpha), we reject the null hypothesis. If the p-value is greater than alpha, we fail to reject the null hypothesis."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the specific statistical test, which is the two-sample t-test, for the following reasons:\n",
        "\n",
        "1.   Comparison of Two Independent Groups: In Hypothesis 1, we are comparing the\n",
        "average counts of rented bikes in two different seasons: summer and winter. The data for these two seasons are independent of each other.\n",
        "\n",
        "2.  Continuous Data: Bike rental counts are a continuous numerical variable, and the t-test is suitable for comparing means of continuous data."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement: There is a significant difference in bike rental counts between weekdays and weekends.**\n",
        "\n",
        "1. **Null Hypothesis (H0): The mean bike count on weekdays is equal to the mean bike count on weekends**.\n",
        "2.**Alternative Hypothesis (H1): The mean bike count on weekdays is not equal to the mean bike count on weekends.**"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Separate the data into two groups: weekdays and weekends\n",
        "weekdays_data = bike_df[bike_df['weekdays_weekend'] == 'Weekday']['Rented_Bike_Count']\n",
        "weekends_data = bike_df[bike_df['weekdays_weekend'] == 'Weekend']['Rented_Bike_Count']\n",
        "\n",
        "# Perform a two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(weekdays_data, weekends_data)\n",
        "\n",
        "# Set the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "    print(\"There is a significant difference in bike rental counts between weekdays and weekends.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis\")\n",
        "    print(\"There is no significant difference in bike rental counts between weekdays and weekends.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hypothesis 2, which tests the difference in bike rental counts between weekdays and weekends.\n",
        "\n",
        "**Two-Sample T-Test:** The t-test is a parametric test that assumes the data follows a normal distribution and has roughly equal variances in the two groups being compared. The p-value was obtained from a two-sample t-test to determine whether there is a significant difference in bike rental counts between weekdays and weekends."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this  specific statistical test for hypothesis 2, which is examining\n",
        "the difference in bike rental counts between different seasons, depends on the nature of the data and the specific characteristics of the research question."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Hypothetical Statement: there is a statistically significant difference in the average bike rental counts between days categorized as \"Functioning Day\" and \"Non-Functioning Day.**\n",
        "*   **Null Hypothesis (H0):** There is no significant difference in the average bike rental counts between functioning days and non-functioning days.\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "* **Alternative Hypothesis (H1):** There is a significant difference in the average bike rental counts between functioning days and non-functioning days.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Separate the data into two groups: Functioning Day and Non-Functioning Day\n",
        "functioning_day_data = bike_df[bike_df['Functioning_Day'] == 'Yes']['Rented_Bike_Count']\n",
        "non_functioning_day_data = bike_df[bike_df['Functioning_Day'] == 'No']['Rented_Bike_Count']\n",
        "\n",
        "# Perform the two-sample t-test\n",
        "t_statistic, p_value = stats.ttest_ind(functioning_day_data, non_functioning_day_data)\n",
        "\n",
        "# Set the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Interpret the result\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "    print(\"There is a significant difference in the average bike rental counts between functioning days and non-functioning days.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis\")\n",
        "    print(\"There is no significant difference in the average bike rental counts between functioning days and non-functioning days.\")\n",
        "\n",
        "# Print the p-value\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nGvosRJ5rLr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   I performed an independent two-sample t-test. This statistical test is used to compare the means of two groups, in this case, the bike rental counts on functioning days and non-functioning days. The p-value obtained from the t-test helps determine whether there is a significant difference in the average bike rental counts between these two groups.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "   **choosing this specific test:**\n",
        "\n",
        " 1. Comparison of Means: In this scenario, we are interested in comparing the average bike rental counts between two independent groups (functioning days and non-functioning days). The t-test is designed for precisely this purpose.\n",
        "\n",
        "2.Continuous Data: The data of interest, which is the bike rental counts, is continuous numerical data. The t-test is appropriate for comparing means of continuous data.\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values\n",
        "\n",
        "missing_values = bike_df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "\n",
        "# & Missing Value Imputation\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
        "#data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Impute missing values with the mean of their respective columns\n",
        "data = bike_df.fillna(bike_df.mean())\n",
        "\n",
        "# Save the imputed dataset to a new file (optional)\n",
        "data.to_csv('imputed_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "fg5EUUavpdmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Missing value imputation** is an important step in data preprocessing. One common method for imputing missing values is to replace them with the mean of the non-missing values in the same column.\n",
        "\n"
      ],
      "metadata": {
        "id": "wCufmyLCp04m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **Mean Imputation for Numerical Columns:**\n",
        "\n",
        "Why: Mean imputation is used when missing values in numerical data are believed to be missing at random, and the mean represents a reasonable estimate for the missing values. It ensures the imputed values maintain the overall distribution of the data.\n",
        "\n",
        "\n",
        "* **Mode Imputation for Categorical Columns:**\n",
        "\n",
        "Why: Mode imputation is suitable for handling missing data in categorical columns when the mode (most frequent category) is a reasonable estimate for missing entries, and you want to maintain the most common category's prevalence in the data.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outliers treatments\n",
        "bike_df.loc[bike_df['Rainfall']>=4,'Rainfall']= 4\n",
        "bike_df.loc[bike_df['Solar_Radiation']>=2.5,'Solar_Radiation']=2.5\n",
        "bike_df.loc[bike_df['Snowfall']>2,'Snowfall']= 2\n",
        "bike_df.loc[bike_df['Wind_speed']>=4,'Wind_speed']= 4\n"
      ],
      "metadata": {
        "id": "LUi3-92kz3PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **we have applied outlier treatment techniques to the dataset by replacing the outliers with the maximum values.**\n",
        "\n"
      ],
      "metadata": {
        "id": "2LGQnl290Myf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying square root to Rented Bike Count to improve skewness\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax=sns.distplot(np.sqrt(bike_df['Rented_Bike_Count']), color=\"y\")\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Mv46pxd0Y_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Since we have generic rule of applying Square root for the skewed variable in order to make it normal .After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "48AkVfWQ0fXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#After applying sqrt on Rented Bike Count check wheater we still have outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=np.sqrt(bike_df['Rented_Bike_Count']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MsOZmbbW03L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **After applying Square root to the Rented Bike Count column, we find that there is no outliers present.**\n",
        "\n"
      ],
      "metadata": {
        "id": "FzbE32X309hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 outliers treatments\n",
        " This is approach to treat outliers in the 'bike_df' DataFrame for columns like 'Rainfall', 'Solar_Radiation', 'Snowfall', and 'Wind_speed'. Specifically, it caps the values of these columns to specific thresholds. This is a method for **handling outliers**, often referred to as winsorization or capping.\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables.\n"
      ],
      "metadata": {
        "id": "AsXcCjN0921e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **One Hot Encoding**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "robfOqZn-LXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "#Assign all categorical features to a variable\n",
        "categorical_features=list(bike_df.select_dtypes(['object','category']).columns)\n",
        "categorical_features=pd.Index(categorical_features)\n",
        "categorical_features\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   A **one hot encoding** allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creat a copy\n",
        "bike_df_copy = bike_df\n",
        "\n",
        "def one_hot_encoding(data, column):\n",
        "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "    data = data.drop([column], axis=1)\n",
        "    return data\n",
        "\n",
        "for col in categorical_features:\n",
        "    bike_df_copy = one_hot_encoding(bike_df_copy, col)\n",
        "bike_df_copy.head()"
      ],
      "metadata": {
        "id": "aWYeka_a-2aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.columns"
      ],
      "metadata": {
        "id": "jdTlba8johz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions\n"
      ],
      "metadata": {
        "id": "y0aPORqTlk4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import contractions\n",
        "\n",
        "# Load your dataset\n",
        "#df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the columns you want to expand contractions in\n",
        "columns_to_expand = ['Seasons', 'Holiday', 'Functioning_Day']\n",
        "\n",
        "# Function to expand contractions in a given text\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# Loop through the specified columns and expand contractions\n",
        "for column_name in columns_to_expand:\n",
        "    bike_df[column_name] = bike_df[column_name].apply(expand_contractions)\n",
        "\n",
        "# Save the updated DataFrame back to a CSV file if needed\n",
        "bike_df.to_csv(\"SeoulBikeData_Expanded.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "anMwjI2UpIuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Expanding contractions** involves converting shortened or contracted forms of words into their full, original forms. This is a common step in text preprocessing for natural language processing (NLP) tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "B4WayGlLCy0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "#df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the columns containing text data that you want to convert to lowercase\n",
        "columns_to_lower = ['Seasons', 'Holiday']\n",
        "\n",
        "# Function to convert text to lowercase\n",
        "def convert_to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Loop through the specified columns and apply lowercasing\n",
        "for column_name in columns_to_lower:\n",
        "    bike_df[column_name] = bike_df[column_name].apply(convert_to_lowercase)\n",
        "\n",
        "# Save the updated DataFrame back to a CSV file if needed\n",
        "bike_df.to_csv(\"SeoulBikeData_Lowercased.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " *   **Lowercasing** is helpful in various NLP tasks, including text classification, sentiment analysis, and text mining, as it ensures that words with different capitalizations are treated as the same word.\n",
        "\n"
      ],
      "metadata": {
        "id": "tSXALe3UDSmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "# Load your dataset\n",
        "#bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the columns containing text data\n",
        "columns_to_process = ['Seasons', 'Holiday', 'Functioning_Day']\n",
        "\n",
        "# Function to remove punctuation from text\n",
        "def remove_punctuation(text):\n",
        "    # Use the string library to get a string of all punctuation characters\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Loop through the specified columns and apply text processing\n",
        "for column_name in columns_to_process:\n",
        "    bike_df[column_name] = bike_df[column_name].apply(remove_punctuation)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "bike_df.to_csv(\"SeoulBikeData_NoPunctuation.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "9PXbPK3SEvtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   To **remove punctuation** from a text in Python, you can use the string library, which provides a string containing all punctuation characters.\n",
        "\n"
      ],
      "metadata": {
        "id": "bM0eyc50E5kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  You can **replace** the text variable with your own text or integrate this code into a larger program or script to remove punctuation from your specific dataset,\n",
        "\n"
      ],
      "metadata": {
        "id": "mgbRxVAkFn89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load your dataset\n",
        "#bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the columns containing text data\n",
        "columns_to_process = ['Seasons', 'Holiday', 'Functioning_Day']\n",
        "\n",
        "# Function to remove URLs from text\n",
        "def remove_urls(text):\n",
        "    # Regular expression to match URLs\n",
        "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "    return re.sub(url_pattern, '', text)\n",
        "\n",
        "# Function to remove words containing digits\n",
        "def remove_words_with_digits(text):\n",
        "    return ' '.join([word for word in text.split() if not any(c.isdigit() for c in word)])\n",
        "\n",
        "# Loop through the specified columns and apply text processing\n",
        "for column_name in columns_to_process:\n",
        "    # Remove URLs\n",
        "    bike_df[column_name] = bike_df[column_name].apply(remove_urls)\n",
        "\n",
        "    # Remove words containing digits\n",
        "    bike_df[column_name] = bike_df[column_name].apply(remove_words_with_digits)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "bike_df.to_csv(\"SeoulBikeData_Cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "xU7R1O-hD0Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   You can use these functions to clean text data that contains URLs and words with digits, such as in the \"SeoulBikeData.csv\" dataset or any other text dataset.\n"
      ],
      "metadata": {
        "id": "6S6UNgBmFYV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download the stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load your dataset\n",
        "#bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the columns containing text data\n",
        "columns_to_process = ['Seasons', 'Holiday', 'Functioning_Day']\n",
        "\n",
        "# Function to remove stopwords and extra white spaces\n",
        "def clean_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    cleaned_tokens = [word for word in tokens if word.lower() not in stopwords.words('english') and word not in string.punctuation]\n",
        "\n",
        "    # Join cleaned tokens with a single space\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "# Loop through the specified columns and apply text processing\n",
        "for column_name in columns_to_process:\n",
        "    bike_df[column_name] = bike_df[column_name].apply(clean_text)\n",
        "\n",
        "# Remove extra white spaces\n",
        "bike_df = bike_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "bike_df.to_csv(\"SeoulBikeData_Cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "kHCy83xfqetW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   You can replace the text variable with your own text or integrate this code into a larger program to remove stopwords from your specific dataset.\n"
      ],
      "metadata": {
        "id": "YsmMV6PNGl3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **To remove white spaces from a text string in Python, you can use the str.replace().**\n",
        "\n"
      ],
      "metadata": {
        "id": "km54uStmHHGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "id": "Ta7SQgvYsQ1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "R_-8vSTn3nAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the text generation model (GPT-2)\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Sample sentence from your dataset\n",
        "sample_sentence = \"The weather today is quite pleasant.\"\n",
        "\n",
        "# Generate a rephrased sentence\n",
        "rephrased_sentence = generator(sample_sentence, max_length=30, num_return_sequences=1)[0]['generated_text']\n",
        "\n",
        "print(\"Original Sentence:\", sample_sentence)\n",
        "print(\"Rephrased Sentence:\", rephrased_sentence)\n"
      ],
      "metadata": {
        "id": "88u3gxIsskxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "FS4Gbb6as-SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required data for NLTK (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load your dataset\n",
        "#bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the column you want to tokenize\n",
        "column_name = \"Seasons\"\n",
        "\n",
        "# Tokenize the text in the specified column\n",
        "bike_df[column_name + '_Tokens'] = bike_df[column_name].apply(lambda text: word_tokenize(text))\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "bike_df.to_csv(\"SeoulBikeData_Tokenized.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "cT2F1A3ZtAiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Tokenization** is essential for various NLP tasks, such as text classification, sentiment analysis, machine translation, and more, as it allows text to be processed and analyzed at a granular level.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jh6U1ZycIskh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load your dataset\n",
        "#bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the column you want to stem\n",
        "column_name = \"Seasons\"\n",
        "\n",
        "# Initialize the PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem the text in the specified column\n",
        "bike_df[column_name + '_Stemmed'] = bike_df[column_name].apply(lambda text: ' '.join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "bike_df.to_csv(\"SeoulBikeData_Stemmed.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "M7fupxKytq5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Stemming** is a process of reducing words to their word stems or root forms. It involves removing common prefixes and suffixes to obtain a common base form. Stemming may result in non-real words or word stems that are not valid words in a language.\n",
        "\n"
      ],
      "metadata": {
        "id": "mGiify4hJF-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required data for NLTK (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load your dataset\n",
        "#bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the column you want to lemmatize\n",
        "column_name = \"Seasons\"\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize the text in the specified column\n",
        "bike_df[column_name + '_Lemmatized'] = bike_df[column_name].apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "bike_df.to_csv(\"SeoulBikeData_Lemmatized.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "mCOWI1gZJ_fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **Lemmatization** is a more sophisticated technique that reduces words to their base forms (lemmas) based on their meaning in a language. It ensures that the resulting word is a valid word in the language. Lemmatization is often considered more accurate than stemming.\n",
        "\n"
      ],
      "metadata": {
        "id": "mwV3gzacKDxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  we start with a sample text containing different forms of words, and we apply **stemming** and **lemmatization** to reduce them to their base forms. You can choose between stemming and lemmatization based on your specific text analysis needs and the level of accuracy required.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n"
      ],
      "metadata": {
        "id": "kKk8ejIdvH50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Load your dataset\n",
        "#bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the column you want to perform POS tagging on\n",
        "column_name = \"Seasons\"\n",
        "\n",
        "# Tokenize the text\n",
        "bike_df[column_name + '_Tokens'] = bike_df[column_name].apply(nltk.word_tokenize)\n",
        "\n",
        "# Perform POS tagging\n",
        "bike_df[column_name + '_POS'] = bike_df[column_name + '_Tokens'].apply(nltk.pos_tag)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "bike_df.to_csv(\"SeoulBikeData_POS_Tagged.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **POS tagging** is essential for various NLP tasks, such as information retrieval, text classification, and named entity recognition, as it provides information about the grammatical role of each word in a sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "WXZA2jjJK5Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Bag of Words (BoW):**\n",
        "\n"
      ],
      "metadata": {
        "id": "h4MHFLZqLVwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load your dataset\n",
        "# bike_df = pd.read_csv(\"SeoulBikeData.csv\")\n",
        "\n",
        "# Specify the column with text data that you want to vectorize\n",
        "column_name = \"Seasons\"\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "X = vectorizer.fit_transform(bike_df[column_name])\n",
        "\n",
        "# Convert the result to a DataFrame\n",
        "bike_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Save the BoW DataFrame to a new CSV file\n",
        "bike_df.to_csv(\"SeoulBikeData_BoW.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have choosed Bag of Words (BoW) vectorization technique Beacause of\n",
        "\n",
        "\n",
        "**Simplicity:** BoW is a straightforward and simple method for text vectorization.\n",
        "\n",
        "**Versatility**: BoW can be used in various NLP tasks, including text classification, sentiment analysis, topic modeling, and more. It provides a foundation for many NLP techniques."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Train Test split for regression\n",
        "\n"
      ],
      "metadata": {
        "id": "WDaEXlfyCvPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "#Assign the value in X and Y\n",
        "X = bike_df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = np.sqrt(bike_df_copy['Rented_Bike_Count'])"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "SLh7rkCyDLNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **Before, fitting any model it is a rule of thumb to split the dataset into a training and test set**. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but mostly used is 80:20 for training and testing respectively. In this step we will split our data into training and testing set using scikit learn library.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "eyjeerkIDsdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create test and train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "G3MoGQ36Dxwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy.info()"
      ],
      "metadata": {
        "id": "SGN0DJOvD4x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*    NO , In regression tasks, you are predicting a numerical value, not class labels. The distribution of the target variable, in this case, \"Rented_Bike_Count,\" is important to understand, but it's not typically referred to as being \"**imbalanced**.\" Instead, you would assess the statistical properties of this numerical variable, such as its mean, median, variance, and the distribution of values.\n",
        "\n",
        "*  for **regression tasks**, it's more common to focus on understanding the characteristics of the target variable's distribution rather than assessing balance, as you would in classification tasks.   \n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The term \"imbalance\" is typically used in the context of classification tasks, where the goal is to categorize data points into classes (e.g., binary classification where you predict \"yes\" or \"no\"). In such cases, a dataset is considered imbalanced when one class significantly outweighs the others. Imbalanced datasets in classification can lead to challenges, such as biased models that are overly influenced by the majority class.\n",
        "\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "vuA57ahBfcBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1   LINEAR REGRESSION"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Regression models describe the relationship between variables by fitting a line to the observed data. Linear regression models use a straight line\n",
        "\n",
        "Linear regression uses a linear approach to model the relationship between independent and dependent variables. In simple words its a best fit line drawn over the values of independent variables and dependent variable. In case of single variable, the formula is same as straight line equation having an intercept and slope.\n",
        "\n",
        "y_pred=β0+β1x\n",
        "\n",
        "where\n",
        "\n",
        "β0 and β1\n",
        "are intercept and slope respectively.\n",
        "\n",
        "In case of multiple features the formula translates into:\n",
        "\n",
        "y_pred=β0+β1x1+β2x2+β3x3+.....\n",
        "\n",
        "where x_1,x_2,x_3 are the features values and\n",
        "β0,β1,β2.....\n",
        "\n",
        "are weights assigned to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent.\n",
        "\n",
        "\n",
        "Gradient descent is the process by which the algorithm tries to update the parameters using a loss function . Loss function is nothing but the diffence between the actual values and predicted values(aka error or residuals). There are different types of loss function but this is the simplest one. Loss function summed over all observation gives the cost functions. The role of gradient descent is to update the parameters till the cost function is minimized i.e., a global minima is reached. It uses a hyperparameter 'alpha' that gives a weightage to the cost function and decides on how big the steps to take. Alpha is called as the learning rate. It is always necesarry to keep an optimal value of alpha as high and low values of alpha might make the gradient descent overshoot or get stuck at a local minima. There are also some basic assumptions that must be fulfilled before implementing this algorithm. They are:\n",
        "\n",
        " 1 .No multicollinearity in the dataset.\n",
        "\n",
        " 2. Independent variables should show linear relationship with dv.\n",
        "\n",
        " 3. Residual mean should be 0 or close to 0.\n",
        "\n",
        " 4. There should be no heteroscedasticity i.e., variance should be constant along the line of best fit.\n",
        "\n"
      ],
      "metadata": {
        "id": "vMOCfEzDKvsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Let us now implement our first model. We will be using LinearRegression from scikit library.\n"
      ],
      "metadata": {
        "id": "CglFdxIgLsOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "39zrrAOQf73L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#Let us now implement our first model. We will be using LinearRegression from scikit library.\n",
        "\n",
        "#import the packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg= LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "#get the X_train and X-test value\n",
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)\n",
        "\n",
        "# Predict on the model\n",
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **Looks like our train set's r2 score value is 0.79 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
        "\n"
      ],
      "metadata": {
        "id": "jkBQTOFsM50M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "reg.score(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "PrzFHxepMYb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the coefficeint\n",
        "reg.coef_"
      ],
      "metadata": {
        "id": "4QHhs0GHMfLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "P8DyReCHNIaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error(y_test, y_pred_test)\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_test, y_pred_test)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score((y_test), (y_pred_test))\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score((y_test), (y_pred_test)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",Adjusted_R2_lr )\n"
      ],
      "metadata": {
        "id": "6R4FXPClNNcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **The test set's r2_score is 0.80. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n"
      ],
      "metadata": {
        "id": "BfVMqKKnNW3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "test_df=pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "236dgae-NdO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBjCdZUKNjQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Heteroscedasticity**\n",
        "\n"
      ],
      "metadata": {
        "id": "c07JmKwgNm5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Heteroscedasticity refers to a situation where the variance of the errors (residuals) is not constant across all levels of the independent variable(s) in a regression model.This violates one of the assumptions of linear regression, which is that the variance of the errors should be constant (homoscedastic) for all levels of the independent variable(s). If the plot shows a funnel shape, with the spread of residuals increasing or decreasing as the predicted values increase, this is an indication of heteroscedasticity.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oxlzWMV5Nujf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity - Residual plot\n",
        "plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V3BxcEqpN1jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual Price vs predicte for Linear Regression plot\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ct2UiFiN-Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **Ridge and Lasso Regression**\n"
      ],
      "metadata": {
        "id": "uUPhHGeqOEUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Ridge and Lasso Regression are types of Regularization techniques\n",
        "\n",
        "*   Regularization techniques are used to deal with overfitting and when the dataset is large\n",
        "* Ridge and Lasso Regression involve adding penalties to the regression function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "POtYfjzeOLsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# * **Lasso Regression**\n"
      ],
      "metadata": {
        "id": "eU2YTBt5OjHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Lasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. It uses the Linear regression model with L1 regularization.\n",
        "\n"
      ],
      "metadata": {
        "id": "A4O0SGDRQtvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "f8Vv1HtMSjVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of Lasso Regression implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha=1.0, max_iter=3000)\n",
        "# Fit the Lasso model\n",
        "lasso.fit(X_train, y_train)\n",
        "# Create the model score\n",
        "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))"
      ],
      "metadata": {
        "id": "HbcKfOIpSnZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_lasso=lasso.predict(X_train)\n",
        "y_pred_test_lasso=lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "Aa1mzrEtSsbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score(y_train, y_pred_train_lasso)\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "_mouXSINSueo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* **Looks like train set's r2 score value is 0.39 that means our model is not able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
        "\n"
      ],
      "metadata": {
        "id": "aqIE6ysmS-OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2)\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "GWDHj74KTG3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_test, y_pred_test_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score((y_test), (y_pred_test_lasso))\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l=(1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "8B5_2CiNTMbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZW8ZikBTdfv"
      },
      "source": [
        "**The test set's r2_score is 0.38. This means our linear model is  not performing well on the data.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "8AtOQGYeTmeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity- Residual plot\n",
        "plt.scatter((y_pred_test_lasso),(y_test-y_pred_test_lasso))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "99asU7DtToUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(np.array(y_pred_test_lasso))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xRiP5itMTwXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **Cross-validation** is a technique used to assess the model's generalization performance. It involves splitting the dataset into multiple subsets (folds), training the model on a subset, and evaluating its performance on the remaining data.\n",
        "\n"
      ],
      "metadata": {
        "id": "lH3rk21zUKpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load your dataset and prepare X (features) and y (target variable)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform k-fold cross-validation (e.g., k=5)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Calculate the root mean squared error (RMSE) from the cross-validation scores\n",
        "rmse_scores = (-cv_scores)**0.5\n",
        "\n",
        "# Print the RMSE scores for each fold\n",
        "print(\"RMSE scores for each fold:\", rmse_scores)\n",
        "\n",
        "# Calculate the average RMSE\n",
        "average_rmse = rmse_scores.mean()\n",
        "print(\"Average RMSE:\", average_rmse)\n"
      ],
      "metadata": {
        "id": "JVSKlwZzUCJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **Hyperparameter tuning** involves optimizing the model's hyperparameters to achieve the best performance. In the case of linear regression, there are not many hyperparameters to tune. However, you can consider regularization hyperparameters, such as alpha for Lasso and Ridge regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "kRSuGy7YUdoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# *   **RIDGE REGRESSION**\n",
        "\n"
      ],
      "metadata": {
        "id": "74FsjyYrUoan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Ridge regression** is a method of estimating the coefficients of regression models in scenarios where the independent variables are highly correlated. It uses the linear regression model with the L2 regularization method.\n",
        "\n"
      ],
      "metadata": {
        "id": "PcuPH1arUwaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "#import the packages\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge= Ridge(alpha=0.1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "ridge.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "ridge.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "dpX_flgOVFhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "#get the X_train and X-test value\n",
        "y_pred_train_ridge=ridge.predict(X_train)\n",
        "y_pred_test_ridge=ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "uSmttywbVHPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_ridge"
      ],
      "metadata": {
        "id": "mTbeT60oVVFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test_ridge"
      ],
      "metadata": {
        "id": "y-DX8sP2VaN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error((y_train), (y_pred_train_ridge))\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_train, y_pred_train_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score(y_train, y_pred_train_ridge)\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "kJ4CZPrUVgp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "*  **Looks like our train set's r2 score value is 0.79 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fl_PBihKVmTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "XTd3aXfCWET4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error(y_test, y_pred_test_ridge)\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score((y_test), (y_pred_test_ridge))\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "IkNAYNoZWKHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **The r2_score for the test set is 0.80. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n"
      ],
      "metadata": {
        "id": "Zrmcar83WPuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "IwS82LiwWYsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity - Residual plot\n",
        "plt.scatter((y_pred_test_ridge),(y_test)-(y_pred_test_ridge))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "acMz_3tCWeyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot((y_pred_test_ridge))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A-fPpEGLWmO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# *   **ELASTIC NET REGRESSION**\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3fwnKn-W8e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **Elastic Net regression** is a linear regression model that combines both  \n",
        "     L1 (Lasso) and L2 (Ridge) regularization penalties to overcome some of the limitations of each individual method.\n",
        "\n",
        "     The model introduces two hyperparameters, alpha and l1_ratio, which control the strength of the L1 and L2 penalties, respectively. Elastic Net regression is particularly useful when dealing with datasets that have high dimensionality and multicollinearity between features.\n",
        "\n"
      ],
      "metadata": {
        "id": "6wAARiNYXGvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import ElasticNet\n",
        "#a * L1 + b * L2\n",
        "#alpha = a + b and l1_ratio = a / (a + b)\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "V7MH3IEmXfE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "elasticnet.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "4iZyX1FRXka4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "jDXXoypsXpMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_en=elasticnet.predict(X_train)\n",
        "y_pred_test_en=elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "VN93kObQXuMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_train_en)\n",
        "print(y_pred_test_en)"
      ],
      "metadata": {
        "id": "R8JECrdVXzEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score(y_train, y_pred_train_en)\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "W6-jQ3JTX4_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Looks like our train set's r2 score value is 0.64 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
        "\n"
      ],
      "metadata": {
        "id": "8XA5BDniX-Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Elastic net regression ',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "SdyfFun4YJ9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_e= mean_squared_error(y_test, y_pred_test_en)\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_test, y_pred_test_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score((y_test), (y_pred_test_en))\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "klZHDUzXYPQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  The r2_score for the test set is 0.63. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).\n",
        "\n"
      ],
      "metadata": {
        "id": "E0K8pH1KYT53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Elastic net regression Test',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "zwsvVoXJYqZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity- Residual plo\n",
        "plt.scatter((y_pred_test_en),(y_test)-(y_pred_test_en))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ay7Kz2M-Yu7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(np.array(y_pred_test_en))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "maODA7_dY1Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In any of the regression model hyperparameter optimization is not used .**"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model - 2      **DECISION TREE**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*    A decision tree is a type of supervised machine learning algorithm that is commonly used for classification and regression tasks. It works by recursively splitting the data into subsets based on the values of certain attributes, ultimately arriving at a set of decision rules that can be used to classify or predict outcomes for new data.\n"
      ],
      "metadata": {
        "id": "mCTeiYSSgM3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "decision_regressor = DecisionTreeRegressor(criterion='friedman_mse', max_depth=8,\n",
        "                      max_features=9, max_leaf_nodes=100,)\n",
        "decision_regressor.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_d = decision_regressor.predict(X_train)\n",
        "y_pred_test_d = decision_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "r3OaMjsDm6ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_train_d)\n",
        "print(y_pred_test_d)"
      ],
      "metadata": {
        "id": "V5zSgyozm8Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",decision_regressor.score(X_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_d= mean_squared_error(y_train, y_pred_train_d)\n",
        "print(\"MSE :\",MSE_d)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_d=np.sqrt(MSE_d)\n",
        "print(\"RMSE :\",RMSE_d)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_d= mean_absolute_error(y_train, y_pred_train_d)\n",
        "print(\"MAE :\",MAE_d)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_d= r2_score(y_train, y_pred_train_d)\n",
        "print(\"R2 :\",r2_d)\n",
        "Adjusted_R2_d=(1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "WUwac-YmnG4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.59, that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "oicHXNnoomYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Dicision tree regression ',\n",
        "       'MAE':round((MAE_d),3),\n",
        "       'MSE':round((MSE_d),3),\n",
        "       'RMSE':round((RMSE_d),3),\n",
        "       'R2_score':round((r2_d),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_d),2)\n",
        "      }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "_jfGxZwRo7U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_d= mean_squared_error(y_test, y_pred_test_d)\n",
        "print(\"MSE :\",MSE_d)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_d=np.sqrt(MSE_d)\n",
        "print(\"RMSE :\",RMSE_d)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_d= mean_absolute_error(y_test, y_pred_test_d)\n",
        "print(\"MAE :\",MAE_d)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_d= r2_score((y_test), (y_pred_test_d))\n",
        "print(\"R2 :\",r2_d)\n",
        "Adjusted_R2_d=(1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "-fygH-UYpBVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.48. This means our linear model is performing  not well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "0zQ0wYuTpF61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Dicision tree regression ',\n",
        "       'MAE':round((MAE_d),3),\n",
        "       'MSE':round((MSE_d),3),\n",
        "       'RMSE':round((RMSE_d),3),\n",
        "       'R2_score':round((r2_d),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_d),2)\n",
        "      }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "Pl8u-gHrpNFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity - Residual plot\n",
        "plt.scatter((y_pred_test_d),(y_test)-(y_pred_test_d))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EJpDtiiVpSUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot((np.array(y_pred_test_d)))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w_uxFy29pcvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the dataset (make sure you have 'SeoulBikeData.csv' in the same directory)\n",
        "bike_df=pd.read_csv('/content/drive/MyDrive/Copy of SeoulBikeData.csv',encoding ='latin')\n",
        "\n",
        "# Define features (X) and the target (y)\n",
        "#get the X_train and X-test value\n",
        "y_pred_train_d = decision_regressor.predict(X_train)\n",
        "y_pred_test_d = decision_regressor.predict(X_test)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor()\n",
        "\n",
        "# Define a hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform Grid Search CV with cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_dt_regressor = grid_search.best_estimator_\n",
        "\n",
        "# Use the best Decision Tree Regressor for prediction\n",
        "y_pred = best_dt_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model as needed (e.g., calculate metrics)\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R-squared (R²): {r2}')\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **I used Grid Search CV as the hyperparameter optimization technique. Here's why Grid Search CV was used and the advantages it offers:**\n",
        "\n",
        " 1.**Comprehensive Search**: Grid Search CV performs an exhaustive search over a predefined set of hyperparameters. It systematically explores all combinations of hyperparameter values, ensuring that no configuration is missed.\n",
        "\n",
        "2. **Ease of Implementation**: Grid Search CV is straightforward to implement, especially when used in conjunction with scikit-learn. It is an essential tool for hyperparameter tuning that is readily available in the scikit-learn library.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **The previous MSE** was higher than 19.6033, it's an improvement.\n",
        "\n",
        "2. **The previous R²** was lower than 0.8726, it's an improvement."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1   **Mean Absolute Error (MAE):**\n",
        "\n",
        "**Indication Towards Business**: MAE measures the average absolute differences between predicted and actual values in regression problems.\n",
        "**Business Impact**: In applications like demand forecasting, lower MAE leads to more accurate predictions and better inventory management.\n",
        "\n",
        "2.  **Mean Squared Error (MSE):**\n",
        "\n",
        "**Indication Towards Business**: MSE measures the average of squared differences between predicted and actual values in regression problems.\n",
        "**Business Impact**: In financial modeling, lower MSE can lead to better risk management and investment decisions.\n",
        "\n",
        "3. **R-squared (R²):**\n",
        "\n",
        "**Indication Towards Business**: R² measures the proportion of variance explained by the model in regression problems.\n",
        "**Business Impact**: In real estate valuation, a higher R² can lead to more accurate property valuations and better investment decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "lSYWKMjtsEsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RANDOM FOREST**"
      ],
      "metadata": {
        "id": "nMgK_RYNti3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Create an instance of the RandomForestRegressor\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "VtviQwJOtsD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_r = rf_model.predict(X_train)\n",
        "y_pred_test_r = rf_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "_C9wpdZnt3kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",rf_model.score(X_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_train, y_pred_train_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_train, y_pred_train_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score(y_train, y_pred_train_r)\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "DBj41sQIt9bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.98 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "HV_4R4hluCQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Random forest regression ',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((r2_rf),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "aTtznnAyuHrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_test, y_pred_test_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_test, y_pred_test_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score((y_test), (y_pred_test_r))\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score((y_test), (y_pred_test_r)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_r)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "U-8RH9t3uN79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.92. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n"
      ],
      "metadata": {
        "id": "Mgxkp49euTBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Random forest regression ',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((r2_rf),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "3WCn7jnSvMFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity- Residual plot\n",
        "plt.scatter((y_pred_test_r),(y_test)-(y_pred_test_r))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Y0X6dIavO_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.feature_importances_"
      ],
      "metadata": {
        "id": "OmpulJOUvZQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURES STORED"
      ],
      "metadata": {
        "id": "7-ZOykVkve_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "Rlt035bKvlcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "atBnWBKSvrb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "_dLHEi06vyQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "o4ulszoOv5Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "A-POzcmCv_Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SojLjtrU1oXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRADIENT BOOSTING**"
      ],
      "metadata": {
        "id": "fDd5AO-410N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation\n",
        "#import the packages\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "\n",
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_g = gb_model.predict(X_train)\n",
        "y_pred_test_g = gb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "S1zlYFMo2TGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* **1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**\n",
        "\n"
      ],
      "metadata": {
        "id": "3wqyWby241XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_model.score(X_train,y_train))\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_train, y_pred_train_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_train, y_pred_train_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score(y_train, y_pred_train_g)\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "ivcudphV2YYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.87 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "26B0Iimk2gue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "g1JHbyEQ2o7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_test, y_pred_test_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_test, y_pred_test_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score((y_test), (y_pred_test_g))\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "X0bK3Z782uev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.86. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "Ay1PG92w2z-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "HYoCyBFZ28Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_g),(y_test)-(y_pred_test_g))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EbKooV6z3Bfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.feature_importances_"
      ],
      "metadata": {
        "id": "KkN8o5d33Ho0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURES STORED"
      ],
      "metadata": {
        "id": "c2EslUJR3MaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "TH4t0f6r3det"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "x1fUUlZ33iKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "i3twcgDO3oBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "umQrhX1y3t0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "8GXWQ-8e35rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "TSTjt_Uy4Azc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Lh231IL4FU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# *   Hyperparameter tuning\n"
      ],
      "metadata": {
        "id": "7uKkTHd_5VrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before proceding to try next models, let us try to tune some hyperparameters and see if the performance of our model improves.**\n",
        "\n",
        "Hyperparameter tuning is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. The key to machine learning algorithms is hyperparameter tuning."
      ],
      "metadata": {
        "id": "5Se8Zwg_5e4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Boosting Regressor with GridSearchCV**"
      ],
      "metadata": {
        "id": "UTfR1zbE6xoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Provide the range of values for chosen hyperparameters**"
      ],
      "metadata": {
        "id": "VtFVHOJ86orh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict"
      ],
      "metadata": {
        "id": "QPtRjwZv6IO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Gradient Boosting Regressor**"
      ],
      "metadata": {
        "id": "xxX82AHs66WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Grid search\n",
        "param_dict = {'learning_rate': [0.1, 0.01],\n",
        "              'n_estimators': [50, 100],\n",
        "              'max_depth': [3, 5]}\n",
        "\n",
        "gb_grid = GridSearchCV(estimator=gb_model,\n",
        "                       param_grid=param_dict,\n",
        "                       cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "gb_grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "z3Yd3Vuv7BkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "F1qEXhio7O6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model = gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "fctGSdc07T84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_params_"
      ],
      "metadata": {
        "id": "xCSfE_y17ZMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_g_g = gb_optimal_model.predict(X_train)\n",
        "y_pred_g_g= gb_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "oPdExNLg7eWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_optimal_model.score(X_train,y_train))\n",
        "MSE_gbh= mean_squared_error(y_train, y_pred_train_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_train, y_pred_train_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score(y_train, y_pred_train_g_g)\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "T3_SJKWa7jgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.94 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "XU8938Bp7ove"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "LGUAsxcj7vwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_gbh= mean_squared_error(y_test, y_pred_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_test, y_pred_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score((y_test), (y_pred_g_g))\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_test, y_pred_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_g_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "xHDv1_xLAASy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter tunning certainly showed a better result, r2 was 0.91 on test and mae and rmse was lowered. Overall model show good result.**"
      ],
      "metadata": {
        "id": "NN-XeyXwA55n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "p4lu_wxaDSWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_g_g),(y_test)-(y_pred_g_g))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fRzkrywjDWvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model.feature_importances_"
      ],
      "metadata": {
        "id": "M4CcbM-QDecY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURES STORED"
      ],
      "metadata": {
        "id": "1P7HYTtYDjWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_optimal_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "8OtjjfmJDpk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "M0MunX8VE9UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "jjdBXgAaFBOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "I6y4ev5yFI2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "GrYqIsPjFQo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "_tItAZ97FV7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WWEXMD05Fb_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using GridSearchCV**\n",
        "\n",
        "GridSearchCV helps to loop through predefined hyperparameters and fit the model on the training set. So, in the end, we can select the best parameters from the listed hyperparameters."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter tunning certainly showed a better result, r2 was 0.91 on test and mae and rmse was lowered. Overall model show good result.**"
      ],
      "metadata": {
        "id": "aSV3zP0T59vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   **Mean Squared Error (MSE) and Mean Absolute Error (MAE):**\n",
        "\n",
        "Why: These metrics are often used in regression problems. Lower MSE and MAE indicate that the model's predictions are closer to the actual values. This can have a positive business impact in applications like demand forecasting or sales prediction by improving inventory management and reducing carrying costs.\n",
        "\n",
        "2.  **R-squared (R²):**\n",
        "\n",
        "Why: R-squared measures the proportion of variance explained by the model. Higher R² indicates a better fit to the data. In business applications such as real estate valuation, a higher R² can lead to more accurate property valuations and better investment decisions.   \n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During our analysis, we conducted an initial exploratory data analysis (EDA) on all the features in our dataset. Firstly, we analysed our dependent variable 'Rented Bike count' and applied transformations as necessar. We then examined the categorical variables and removed those with majority of one class. We also studied the numerical variables, calculated their correlations, distribution and the their relationships with the dependent variable. Additionally we removed some numerical features that contained mostly 0 values and applied one-hot encoding to the categorical variables.\n",
        "Subsequently, we employed 7 machine learning algorithms including Linear Regression,Lasso , Ridge, Elastic Net, Decision Tree, Random Forest and Gradient Booster. We also performed hyperparameter tuning to enhance the performance of our models. The evaluation of our models resulted in the following findings :"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the results of evaluation metric values for all models\n",
        "result=pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
        "result"
      ],
      "metadata": {
        "id": "6xQXkubyJrp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train a model to predict the number of rented bike count in given weather conditions. First, we do Exploratory Data Analysis on the data set. We look for null values that is not found in dataset and outliers and appropriately modify them. We also perform correlation analysis to extract out the important and relevant feature set and later perform feature engineering.\n",
        "\n",
        "\n",
        "*  Gradient Boosting gridsearchcv model shows promising result with R2 score of 0.91, therefore it can be used to solve this problem.\n",
        "\n",
        "*  Temperatue, Functioning_Day_Yes, Humidity, Rainfall and Solar radiation are major driving factors for the Bike rent demand.\n",
        "\n",
        "*  Bike demand shows peek around 8-9 AM in the morning and 6 - 7pm in the evening.\n",
        "\n",
        "*  People prefer to rent bike more in summer than in winter.\n",
        "\n",
        "*  Bike demand is more on clear days than on snowy or rainy days.\n",
        "\n",
        "\n",
        "*  Temperature range from 22 to 25(°C) has more demand for bike.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BNljJFXoJ2FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the current analysis may be insightful, it is important to note that the dataset is time-dependent and variables such as temperature, windspeed and solar radiation may not always remain consistent. As a result there may be situations where the model fails to perform well. As field of machine learning is constantly evolving, it is necessary to stay up-to-date with the latest developments and be prepared to handle unexpected scenarios. Maintaining a strong understanding of Machine Learning concepts will undoubtely provide an advantage in staying ahead in the future."
      ],
      "metadata": {
        "id": "3QG5PJVGKm1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}